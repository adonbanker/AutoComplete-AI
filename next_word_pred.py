# -*- coding: utf-8 -*-
"""Next_word_Pred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1saBXCAEP7nO-tvMIsfByHDmpjR1Av2E_
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

import kagglehub

path = kagglehub.dataset_download("muhammadbilalhaneef/sherlock-holmes-next-word-prediction-corpus")

print("Path to dataset files:", path)

import os
file_path = os.path.join(path, 'Sherlock Holmes.txt')
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

print("Text file loaded successfully!")

import os
print(os.listdir(path))

text

mytokenizer = Tokenizer()
mytokenizer.fit_on_texts([text])
total_words = len(mytokenizer.word_index) + 1

mytokenizer.word_index



myinput = []
for line in text.split("\n"):
  token_list = mytokenizer.texts_to_sequences([line])[0]
  for i in range(1,len(token_list)):
    my_n_gram_sequence = token_list[:i+1]
    myinput.append(my_n_gram_sequence)



# Pad sequences
max_sequence_length = max([len(seq) for seq in myinput])
padded_sequences = pad_sequences(myinput, maxlen=max_sequence_length, padding='pre')

# Create features and labels
X = padded_sequences[:, :-1]
y = padded_sequences[:, -1]

# Convert labels to one-hot encoding
y = tf.keras.utils.to_categorical(y, num_classes=total_words)

import os
import kagglehub
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Download and load the dataset
path = kagglehub.dataset_download("muhammadbilalhaneef/sherlock-holmes-next-word-prediction-corpus")
file_path = os.path.join(path, 'Sherlock Holmes.txt')
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# Generate n-gram sequences
myinput = []
for line in text.split("\n"):
  token_list = mytokenizer.texts_to_sequences([line])[0]
  for i in range(1,len(token_list)):
    my_n_gram_sequence = token_list[:i+1]
    myinput.append(my_n_gram_sequence)

# Ensure necessary variables are defined
mytokenizer = Tokenizer()
mytokenizer.fit_on_texts([text])
total_words = len(mytokenizer.word_index) + 1

max_sequence_length = max([len(seq) for seq in myinput])


# Define the model
model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_sequence_length - 1))
model.add(LSTM(150))
model.add(Dense(total_words, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Print the model summary
model.summary()

import os
import kagglehub
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Download and load the dataset
path = kagglehub.dataset_download("muhammadbilalhaneef/sherlock-holmes-next-word-prediction-corpus")
file_path = os.path.join(path, 'Sherlock Holmes.txt')
with open(file_path, 'r', encoding='utf-8') as file:
    text = file.read()

# Generate n-gram sequences
myinput = []
for line in text.split("\n"):
  token_list = mytokenizer.texts_to_sequences([line])[0]
  for i in range(1,len(token_list)):
    my_n_gram_sequence = token_list[:i+1]
    myinput.append(my_n_gram_sequence)

# Ensure necessary variables are defined
mytokenizer = Tokenizer()
mytokenizer.fit_on_texts([text])
total_words = len(mytokenizer.word_index) + 1

max_sequence_length = max([len(seq) for seq in myinput])

# Pad sequences
padded_sequences = pad_sequences(myinput, maxlen=max_sequence_length, padding='pre')

# Create features and labels
X = padded_sequences[:, :-1]
y = padded_sequences[:, -1]

# Convert labels to one-hot encoding
y = tf.keras.utils.to_categorical(y, num_classes=total_words)

# Train the model
model.fit(X, y, epochs=25, verbose=1)

import numpy as np

def generate_text(seed_text, next_words, model, max_sequence_length):
    for _ in range(next_words):
        token_list = mytokenizer.texts_to_sequences([seed_text])[0]
        token_list = pad_sequences([token_list], maxlen=max_sequence_length - 1, padding='pre')
        predicted_probabilities = model.predict(token_list, verbose=0)
        predicted_word_index = np.argmax(predicted_probabilities)

        output_word = ""
        for word, index in mytokenizer.word_index.items():
            if index == predicted_word_index:
                output_word = word
                break
        seed_text += " " + output_word
    return seed_text

# Example usage:
seed_text = "I saw a door"
generated_text = generate_text(seed_text, 15, model, max_sequence_length)
print(generated_text)